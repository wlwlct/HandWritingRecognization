{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to differenciate 0 and o\n",
    "- How to differenciate number with decimal or not\n",
    "- terms of fraction?\n",
    "- How do you separate each character from a connected string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import sobel\n",
    "from sklearn.model_selection import train_test_split, learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "- mnist: the mnist database of handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impot datasets\n",
    "(digits_x_train, digits_y_train), (digits_x_test, digits_y_test) = keras.datasets.mnist.load_data() #Tuple of NumPy arrays: (x_train, y_train), (x_test, y_test)\n",
    "# examine data\n",
    "plt.imshow(digits_x_train[2,:,:])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append 2 dataset\n",
    "digits_x = np.concatenate((digits_x_train, digits_x_test), axis=0)\n",
    "digits_y = np.concatenate((digits_y_train, digits_y_test), axis=0)\n",
    "digits_x.shape, digits_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impot datasets\n",
    "letters = np.loadtxt('A_Z Handwritten Data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine data\n",
    "n=45000\n",
    "print(letters[n,0])\n",
    "plt.imshow(letters[n,1:].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_x = letters[:,1:].reshape(-1,28,28)\n",
    "letters_y = letters[:,0]+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((digits_x,letters_x), axis=0)\n",
    "y = np.concatenate((digits_y,letters_y), axis=0)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization, Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation # width_shift # height_shift # sheer range # zoom range\n",
    "def GenTrans(imgs, labels, rotation=0, width_shift=0, height_shift=0, sheer=0, zoom=0):\n",
    "    # allocate y and imgs\n",
    "    imgs_leng = sum((len(np.arange(-rotation, rotation+1, 5)), \\\n",
    "                    len(np.arange(-width_shift*28, width_shift*28,1)), \\\n",
    "                    len(np.arange(-height_shift*28, height_shift*28,1)), \\\n",
    "                    len(np.arange(-sheer, sheer, 0.05)), \\\n",
    "                    len(np.arange(1-zoom, 1+zoom, 0.05))))\n",
    "    imgs_output = np.empty((imgs_leng*imgs.shape[0], 28,28))\n",
    "    labels_output = np.empty((imgs_leng*imgs.shape[0],1))\n",
    "\n",
    "    imgs_range = range(imgs.shape[0])\n",
    "    #imgs_output = np.empty((0,28,28))\n",
    "    #labels_output = np.empty((0,1))\n",
    "    \n",
    "    for i in imgs_range:\n",
    "        #print('*',i, i*imgs_leng, (i+1)*imgs_leng)\n",
    "        imgs_output[i*imgs_leng:(i+1)*imgs_leng,:,:] = GenTrans_singleImg(imgs[i,:,:], rotation, width_shift, height_shift, sheer, zoom)\n",
    "        labels_output[i*imgs_leng:(i+1)*imgs_leng,:] = np.ones((imgs_leng,1))*labels[i]\n",
    "\n",
    "        #imgs_output = np.concatenate((imgs_output, img_aug), axis=0)\n",
    "        #labels_output = np.concatenate((labels_output, np.ones((img_aug.shape[0],1))*labels[i]), axis=0)\n",
    "    return imgs_output, labels_output\n",
    "\n",
    "\n",
    "def GenTrans_singleImg(img, rotation=0, width_shift=0, height_shift=0, sheer=0, zoom=0):\n",
    "    # normalization\n",
    "    img=img/255\n",
    "    # rotation\n",
    "    rotation_range = np.arange(-rotation, rotation+1, 5)\n",
    "    rotation_img = np.zeros((len(rotation_range), img.shape[0], img.shape[1]))\n",
    "    for i,angle in enumerate(rotation_range):\n",
    "        rotation_img[i,:,:] = transform.rotate(img, angle=angle)\n",
    "    \n",
    "\n",
    "    # width_shift\n",
    "    width_range = np.arange(-width_shift*img.shape[1], width_shift*img.shape[1],1)\n",
    "    width_img = np.zeros((len(width_range), img.shape[0], img.shape[1]))\n",
    "    for i,w in enumerate(width_range):\n",
    "        width_img[i,:,:]=transform.warp(img, transform.AffineTransform(translation=(w,0)), mode='wrap')\n",
    "    \n",
    "    \n",
    "    # height_shift\n",
    "    height_range = np.arange(-height_shift*img.shape[1], height_shift*img.shape[1],1)\n",
    "    height_img = np.zeros((len(height_range), img.shape[0], img.shape[1]))\n",
    "    for i,h in enumerate(height_range):\n",
    "        height_img[i,:,:]=transform.warp(img, transform.AffineTransform(translation=(0,h)), mode='wrap')\n",
    "    \n",
    "    # sheer\n",
    "    sheer_range = np.arange(-sheer,sheer, 0.05)\n",
    "    sheer_img = np.zeros((len(sheer_range), img.shape[0], img.shape[1]))\n",
    "    for i,s in enumerate(sheer_range):\n",
    "        sheer_img[i,:,:]=transform.warp(img, transform.AffineTransform(shear=s), order=1, preserve_range=True, mode='wrap')\n",
    "    \n",
    "    # zoom\n",
    "    zoom_range = np.arange(1-zoom, 1+zoom, 0.05)\n",
    "    zoom_img = np.zeros((len(zoom_range), img.shape[0], img.shape[1]))\n",
    "    for i,z in enumerate(zoom_range):\n",
    "        if z<=1:\n",
    "            zoom_img[i,:,:] = transform.resize(transform.rescale(img,z), (28,28))\n",
    "        else:\n",
    "            img_transformed = transform.rescale(img,z)\n",
    "            zoom_img[i,:,:] = img_transformed[img_transformed.shape[0]//2-14:img_transformed.shape[0]//2+14, \\\n",
    "                                              img_transformed.shape[1]//2-14:img_transformed.shape[1]//2+14]\n",
    "    \n",
    "    img_all=np.empty((0,28,28))\n",
    "    for i in [rotation_img, width_img, height_img, sheer_img, zoom_img]:\n",
    "        #print(img_all.shape, i.shape)\n",
    "        img_all = np.concatenate((img_all,i), axis=0)\n",
    "    \n",
    "    return img_all\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Genfeature(img):\n",
    "    img = img.reshape(28,28)    \n",
    "    img_features = np.array([])     \n",
    "    # FEATURE 1 - Pixel values\n",
    "    #Add pixel values to the data frame\n",
    "    pixel_values = img.reshape(-1)\n",
    "    img_features = np.concatenate((img_features,pixel_values), axis=0)\n",
    "\n",
    "    # FEATURE 2 - Bunch of Gabor filter responses\n",
    "    #Generate Gabor features\n",
    "    for theta in range(8):   #Define number of thetas\n",
    "        theta = theta / 4. * np.pi\n",
    "        for sigma in (1, 5):  #Sigma with 1 and 3\n",
    "            lamda = np.pi/4\n",
    "            gamma = 0.5\n",
    "            ksize=9\n",
    "            kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma, 0, ktype=cv2.CV_32F)    \n",
    "            \n",
    "            #Now filter the image and add values to a new column \n",
    "            fimg = cv2.filter2D(img, cv2.CV_8UC3, kernel)\n",
    "            filtered_img = fimg.reshape(-1)\n",
    "            img_features = np.concatenate((img_features,filtered_img), axis=0)\n",
    "     \n",
    "    # FEATURE 3 Sobel\n",
    "    edge_sobel = sobel(img)\n",
    "    edge_sobel = edge_sobel.reshape(-1)\n",
    "    img_features = np.concatenate((img_features,edge_sobel), axis=0)\n",
    "\n",
    "    #print('pixel_values.shape: ', filtered_img.shape)\n",
    "\n",
    "    return img_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_features(train_X, train_y):\n",
    "    # allocate\n",
    "    train_X_features = np.zeros((len(train_y), len(Genfeature(train_X[0]))))\n",
    "    \n",
    "    # go through each entry\n",
    "    for i,k in enumerate(train_X):\n",
    "        train_X_features[i,:] = Genfeature(k)\n",
    "    return train_X_features\n",
    "\n",
    "def sample_split(X, y):\n",
    "    # Under sampling\n",
    "    US = RandomUnderSampler(random_state=42)\n",
    "    X_res, y_res = US.fit_resample(X.reshape(len(y),-1),y)\n",
    "\n",
    "    # Augmentation\n",
    "    #X_res, y_res= GenTrans(X_res.reshape(-1,28,28), y_res, width_shift=0.1, height_shift=0.1, zoom=0.1)\n",
    "\n",
    "\n",
    "    # Train test split\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X_res.reshape(len(y_res),-1), y_res, random_state=42, train_size=0.8)\n",
    "\n",
    "    print('-'*50)\n",
    "    print('Shape of train X, test_X, train_y, test_y, Feature: \\n')\n",
    "    for i in [train_X, test_X, train_y, test_y]:\n",
    "        print(i.shape)\n",
    "    print(Genfeature(train_X[0]).shape)\n",
    "    print('-'*50+'\\n')\n",
    "\n",
    "    \n",
    "    # Generate features\n",
    "    train_X_features = X_features(train_X, train_y)\n",
    "    test_X_features = X_features(test_X, test_y)\n",
    "\n",
    "    return train_X_features, test_X_features, train_y, test_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building (letter and string together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_features, test_X_features, train_y, test_y = sample_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=36)\n",
    "pca_class = pca.fit_transform(train_X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_class[:,0], pca_class[:,1], c=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(train_X_features, train_y)\n",
    "pred_y = rfc.predict(test_X_features)\n",
    "print(classification_report(y_pred=pred_y, y_true=test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_train_sizes_both, rfc_train_scores_both, rfc_test_scores_both = learning_curve(rfc, train_X_features, train_y, train_sizes=np.array([0.1, 0.33, 0.55, 0.78, 1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rfc_train_sizes_both, -rfc_test_scores_both.mean(1), \"o--\", color=\"g\", label=\"KRR\")\n",
    "plt.xlabel(\"Train size\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Learning curves\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_pred=pred_y, y_true=test_y), cmap=\"YlGnBu\", vmax=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_svc = SVC(random_state=42)\n",
    "md_svc.fit(train_X_features, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_y = md_svc.predict(test_X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred=svc_y, y_true=test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_train_sizes_both, svc_train_scores_both, svc_test_scores_both = learning_curve(md_svc, train_X_features, train_y, train_sizes=np.array([0.1, 0.33, 0.55, 0.78, 1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building (letter and string separately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_train_X_features, digits_test_X_features, digits_train_y, digits_test_y = sample_split(digits_x,digits_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_train_X_features, letters_test_X_features, letters_train_y, letters_test_y = sample_split(letters_x,letters_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_rfc = RandomForestClassifier(random_state=42)\n",
    "digits_rfc.fit(digits_train_X_features, digits_train_y)\n",
    "digits_pred_y = digits_rfc.predict(digits_test_X_features)\n",
    "print(classification_report(y_pred=digits_pred_y, y_true=digits_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_train_sizes_digits, rfc_train_scores_digits, rfc_test_scores_digits = learning_curve(digits_rfc, digits_train_X_features, digits_train_y, train_sizes=np.array([0.1, 0.33, 0.55, 0.78, 1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_rfc = RandomForestClassifier(random_state=42)\n",
    "letters_rfc.fit(letters_train_X_features, letters_train_y)\n",
    "letters_pred_y = letters_rfc.predict(letters_test_X_features)\n",
    "print(classification_report(y_pred=letters_pred_y, y_true=letters_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_train_sizes_letters, rfc_train_scores_letters, rfc_test_scores_letters = learning_curve(letters_rfc, digits_train_X_features, digits_train_y, train_sizes=np.array([0.1, 0.33, 0.55, 0.78, 1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "from imutils.contours import sort_contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('Example/images/hello_world.png')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "edged = cv2.Canny(blurred, 30, 150)\n",
    "cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,\n",
    "\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "cnts = sort_contours(cnts, method=\"left-to-right\")[0]\n",
    "\n",
    "# initialize the list of contour bounding boxes and associated\n",
    "# characters that we'll be OCR'ing\n",
    "chars = []\n",
    "\n",
    "# loop over the contours\n",
    "for c in cnts:\n",
    "\t# compute the bounding box of the contour\n",
    "\t(x, y, w, h) = cv2.boundingRect(c)\n",
    "\n",
    "\t# filter out bounding boxes, ensuring they are neither too small\n",
    "\t# nor too large\n",
    "\tif (w >= 5 and w <= 150) and (h >= 15 and h <= 120):\n",
    "\t\t# extract the character and threshold it to make the character\n",
    "\t\t# appear as *white* (foreground) on a *black* background, then\n",
    "\t\t# grab the width and height of the thresholded image\n",
    "\t\troi = gray[y:y + h, x:x + w]\n",
    "\t\tthresh = cv2.threshold(roi, 0, 255,\n",
    "\t\t\tcv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\t\t(tH, tW) = thresh.shape\n",
    "\n",
    "\t\t# if the width is greater than the height, resize along the\n",
    "\t\t# width dimension\n",
    "\t\tif tW > tH:\n",
    "\t\t\tthresh = imutils.resize(thresh, width=32)\n",
    "\n",
    "\t\t# otherwise, resize along the height\n",
    "\t\telse:\n",
    "\t\t\tthresh = imutils.resize(thresh, height=32)\n",
    "\n",
    "\t\t# re-grab the image dimensions (now that its been resized)\n",
    "\t\t# and then determine how much we need to pad the width and\n",
    "\t\t# height such that our image will be 32x32\n",
    "\t\t(tH, tW) = thresh.shape\n",
    "\t\tdX = int(max(0, 32 - tW) / 2.0)\n",
    "\t\tdY = int(max(0, 32 - tH) / 2.0)\n",
    "\n",
    "\t\t# pad the image and force 32x32 dimensions\n",
    "\t\tpadded = cv2.copyMakeBorder(thresh, top=dY, bottom=dY,\n",
    "\t\t\tleft=dX, right=dX, borderType=cv2.BORDER_CONSTANT,\n",
    "\t\t\tvalue=(0, 0, 0))\n",
    "\t\tpadded = cv2.resize(padded, (28, 28))\n",
    "\n",
    "\t\t# prepare the padded image for classification via our\n",
    "\t\t# handwriting OCR model\n",
    "\t\tpadded = padded.astype(\"float32\") / 255.0\n",
    "\t\tpadded = np.expand_dims(padded, axis=-1)\n",
    "\n",
    "\t\t# update our list of characters that will be OCR'd\n",
    "\t\tchars.append((padded, (x, y, w, h)))\n",
    "# extract the bounding box locations and padded characters\n",
    "boxes = [b[1] for b in chars]\n",
    "chars = np.array([c[0] for c in chars], dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {i:str(i) for i in range(10)}\n",
    "for i in range(10,36,1):\n",
    "    label[i] = chr(i+55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(chars.reshape(-1,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[label[i] for i in md_svc.predict(chars.reshape(10,-1)*255)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[label[i] for i in rfc.predict(chars.reshape(10,-1)*255)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
